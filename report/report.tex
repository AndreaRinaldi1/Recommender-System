\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}



\begin{document}
\title{Computational Intelligence Laboratory Project}

\author{
  Andrea Rinaldi, Simon Haefeli, Giuseppe Russo, Gianni Perlini\\
  Group Name: Team Name \\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}

Recommender Systems are used by big companies such as Netflix and Amazon to perform targeted advertising. A common technique used to design such systems is Collaborative Filtering (CF), which works by analysing data on users' information such as, for example, movies' rating and preferences in terms of items. In this project we implement 7 different models for CF and then combine them using ensemble learning to further improve the results of our predictions. We use Singular Value Decomposition (SVD) and SVD with Stochastic Gradient Descent (SGD) as our baselines and compare our improvements to them. Some of the methods we used includes Bayesian Probabilistic Matrix Factorization (BPMF) and Non-parametric Principal Component Analysis (NPCA). The results are obtained using the provided dataset. %TODO add final improvement of our model wrt the baseline

\end{abstract}

\section{Introduction}

Recommender systems are used in multiple situations in today's Internet. They can be found in e-commerce applications to recommend items to purchase, or in on-line video streaming to give advice on which movie to watch next. An example of companies using these systems are Netflix and Amazon. These types of companies are confronted with millions of users and items. This gives rise to the need of having a recommender system which is both reliable and efficient, while dealing with an increasing amount of data. A common technique to design recommender systems is collaborative filtering. This technique uses a user's past behaviour (preferences, ratings, ...) to make predictions. Common approaches in collaborative filtering are memory based techniques such as user-based, measuring the similarity between a given user and other users, and item-based, which measures the similarity between the selected item and other ones. Other types of CF are model-based techniques such as PCA and SVD. These different methods have widely been studied during the past years and all come with their strengths and limitations. To overcome such weaknesses, a hybrid approach is often used in order to combine multiple models. This allows the recommender system to benefit from the pros of different techniques while minimizing the limitations of each one of them.\\
In this paper we use this third approach to implement our own recommender system. We use 7 different models and then combine all of them in a hybrid one using \textbf{M}ulti \textbf{L}ayer \textbf{P}erceptron (MLP) and Ridge regressions. .\\
Our paper is structured as follows: in \emph{Section \ref{mam}} we present the mathematical description of the models and give some details about their implementations. In \emph{Section \ref{res}} we show the results obtained by performing prediction with our approach, and we discuss these results in \emph{Section \ref{disc}}. We conclude our paper in \emph{Section \ref{conc}}.

\section{Models and Methods}
\label{mam}

In this section we describe the problem we are tackling as well as the models we used to solve it.

\subsection{Problem Description}

We are given a dataset of 10000 users and 1000 movies. Our goal is to predict the rating $r_{ij}$ given by user $i$ to the $j$-th movie. All ratings are integer numbers between 1 and 5. The evaluation of the predictions is done according to the \textbf{R}oot \textbf{M}ean \textbf{S}quared \textbf{E}rror (RMSE), which is defined as:

$$
RMSE = \sqrt{\frac{1}{\vert N \vert} \sum_{(i,j)\in N} (r_{ij} - \hat{r}_{ij}})^2
$$

\noindent where $r_{ij}$ is the true rating, $\hat{r}_{ij}$ is the predicted one and $N$ is the set of user-movie pairs that we want to evaluate.

\subsection{Models Description}

%TODO add implementations' params

\begin{description}

\item[\emph{User-based with Pearson}]\ \\
This method uses similarity between users' ratings to recommend items to the target user. As a measure of similarity we use Pearson's correlation, which is defined as follows [Paper]:

$$
sim_p(i, j) = \frac{\sum_{u \in U} (r_{i,u} - \bar{r_i})(r_{j,u} - \bar{r_j})}{\sqrt{\sum_{u \in U} (r_{i,u} - \bar{r_i})^2}	\sqrt{\sum_{u \in U} (r_{j,u} - \bar{r_j})^2}}
$$

where $U$ represents the set of items that have been rated by both users $i$ and $j$, and $\bar{r_i}$ (resp. $\bar{r_j}$) is the average over the ratings of user $i$ (resp. $j$). The result is a value comprised in $[-1;1]$. A -1 indicates that the ratings of the considered users are opposite, while a result of 1 indicates that they are similar. A value of 0 means that the two ratings seems to be independent. \\
Once we have the similarity coefficients, the predicted rating is calculated as follows:

$$
\hat{r_{iu}} = \frac{\sum_{s \in S_u} (sim_p(s, i) \ast r_{si} )}{\sum_{s \in S_u} \vert sim_p(s, i) \vert}
$$ 

\item[\emph{Regularized SVD}]\ \\
This method combines SVD with regularized SGD to make predictions on users' ratings. The data matrix is first decomposed into two matrices $\textbf{U} = \left[ u_i \right]$ and $\textbf{V} = \left[ v_j \right]$ representing the user and item latent feature matrices where $u_i,v_j \in \mathbb{R}^k$. The predicted rating is then given by $\hat{r}_{ij} = u_i^Tv_j$. We then use SGD with regularization to estimate $u$ and $v$ as follows [Paterek paper]:

$$
\begin{aligned}
& e_{ij} = r_{ij} - \hat{r}_{ij} \\
& u_{ik} += \eta \ast (e_{ij}v_{jk} - \lambda u_{ik}) \\
& v_{jk} += \eta \ast (e_{ij}u_{ik} - \lambda v_{jk})
\end{aligned}
$$

\noindent where $e_{ij}$ is the error between the true rating $r_{ij}$ and the predicted one $\hat{r_{ij}}$, $\eta$ is the learning rate and $\lambda$ is the regularization term.


\item[\emph{Post-processing SVD with Kernel Ridge Regression}] \ \\
This method combines ridge regression with the kernel trick to improve SVD. To do that, the $u_{ik}$ weights are discarded and the $v_{jk}$ are used as predictors. Defining $y$ as the vector of movies rated by user $i$ and $X$ as the matrix of observation, the prediction of $y$ by ridge regression is given by [Paterek paper]:

$$
\hat{y}_i = x_i^T(X^TX + \lambda I)^{-1}X^Ty
$$

\noindent Using an equivalent formulation involving Gram matrix $XX^T$ and the kernel trick, we get the following prediction:

$$
\hat{y}_i = K(x_i^T,X)(K(X,X) + \lambda I)^{-1}y
$$

\noindent where $K$ is, in our case, a Gaussian kernel defined as $K(x_i^T, x_j^T) = \exp(2(x_i^Tx_j-1))$ and $\lambda = .7$.
 
 
\item[\emph{k-means}]\ \\
The k-means algorithm is used in collaborative filtering to divide users into $K$ clusters $C_k$ and minimizing the intra-cluster variance, defined as [Paterek paper]:

$$
\sum_{k=1}^K \sum_{i \in C_k} \Vert y_i - \mu_k \Vert^2 = \sum_{k=1}^K \sum_{i \in C_k} \sum_{j \in J_i} (y_{ij} - \mu_{kj})^2
$$

\noindent where $J_i$ is the set of movies rated by user $i$ and $\mu_{kj}$ is the prediction for rating of each user in $C_k$ given to movie $j$.

%The k-means method is used for clustering n data points into k clusters. We use this method to partition the set of users into k clusters. The objective we want to minimize is defined as follows:

%$$ 
%\sum_{i=1}^N \sum_{j=1}^K (z_{ij} \Vert \textbf{r}_i - \textbf{u}_j \Vert^2) 
%$$

%\noindent where $z_{ij}$ indicates whether user $i$ is in cluster $j$ or not, $\Vert \textbf{r}_i - \textbf{u}_j \Vert^2)$ is computed for all movies rated by user $i$, with $\textbf{r}_i$ being the actual rating and $\textbf{u}_j$ the predicted one.	

\item[\emph{BPMF}] \ \\
This method aims at improving Probabilistic Matrix Factorization techniques by taking a Bayesian approach, and is described in [BPMF paper]. In BPMF, the conditional distribution over the observed ratings $R$ and the prior distributions over user-specific and movie-specific latent feature matrices $U$ and $V$ are assumed to be Gaussian, whereas the user and movie hyper parameters are assumed to follow a Wishart-Gaussian distribution. The prediction of rating $R_{ij}^*$ is given by

$$
\begin{aligned}
p(R_{ij}^* \vert R, \Theta_0) =  \int\int p(R_{ij}^* \vert U_i,V_j)p(U,V \vert R, \Theta_U \Theta_V) \\
p(\Theta_U \Theta_V \vert \Theta_0)d \left\lbrace U,V \right\rbrace  d \left\lbrace \Theta_U, \Theta_V \right\rbrace
\end{aligned}
$$

\noindent However, this evaluation is intractable. The authors thus suggest the use of a MCMC-based method to approximate the above integrals. In particular, they use the Gibbs sampling algorithm to implement BPMF. \emph{Figure \ref{gibbs}} shows the Gibbs algorithm as shown in [BPMF paper].

\begin{figure}[h!]
\centering
\includegraphics[scale=0.35]{gibbs.png}
\caption{Gibbs sampling algorithm as shown in [BPMF paper]}
\label{gibbs}
\end{figure} 

\item[\emph{NPCA}] \ \\
NPCA is an extension of Probabilistic PCA which deals with infinitely many latent factors. We implemented the model following [NPCA paper]. In this situation, working directly with the vector of latent factors $\textbf{u}$ and $\textbf{v}$ is intractable and the vector $\textbf{x}_i = \left[u_i^Tv_1, \dots , u_i^Tv_j, \dots , u_i^Tv_N \right] $ is used instead. The model then describes a latent process $X$ and an observational one $Y$ described by the following distribution:
$$
\int p(Y,X \vert K, \lambda)dX = \prod_{i=1}^M \mathcal{N}(y_{\mathbb{O}_i};0, K_{\mathbb{O}_i} + \lambda I)
$$

\noindent where we keep the same notation as in [NPCA paper] and which is maximized using an EM algorithm. The authors formalize a first EM algorithm which is however computationally too costly, and thus they suggest an optimized one which is depicted in \emph{Figure \ref{emnpca}}.

\begin{figure}[h!]
\centering
\includegraphics[scale=0.6]{emnpca.png}
\caption{Fast NPCA EM algorithm as shown in [NPCA paper]}
\label{emnpca}
\end{figure} 

\item[Auto-encoder]


\end{description}

\subsection{Ensemble}

\section{Evaluation}
\label{res}

\subsection{Set-up}

Our dataset comprises 10000 users and 1000 movies. The results are computed by submitting the predictions to the Kaggle platform, which, as mentioned in \emph{Problem Description}, evaluates the prediction error via RMSE.

\subsection{Results}

We present the results of single models in \emph{Table \ref{tabres}} and of the two types of ensemble methods in \emph{Table \ref{ensres}}. We discuss them in the following section.

\begin{table}[h!]
	\centering
	\begin{tabular}{l|l}
		\textit{\textbf{Model}} & \textit{\textbf{RMSE}} \\
		\hline
		SVD                     &                        \\
		SVD + SGD               &                        \\
		RSVD       	            &                        \\
		Ridge SVD               &                        \\
		User-Based              &                        \\
		k-means                 &                        \\
		BPMF                    &                        \\
		NPCA                    &                        \\
		AutoEncoder             &                       
	\end{tabular}
\caption{Results of our models' evaluation}
\label{tabres}
\end{table}

\begin{table}[h!]
	\centering
	\begin{tabular}{l|l}
		\textit{\textbf{Ensemble}} & \textit{\textbf{RMSE}} \\
		\hline
		Ridge                     &                        \\
		MLP      	            &                                            
	\end{tabular}
\caption{Results of our ensemble methods' evaluation}
\label{ensres}
\end{table}

\section{Discussion}
\label{disc}


\section{Conclusions}
\label{conc}



%\bibliographystyle{IEEEtran}
 %\bibliography{report}
\end{document}
